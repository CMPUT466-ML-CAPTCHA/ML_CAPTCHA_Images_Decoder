{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python385jvsc74a57bd09e57da381150c32bc445a617f5277ea2102eb0088f85f33ec3bf81d0c63bfe54",
      "display_name": "Python 3.8.5 64-bit ('base': conda)"
    }
  },
  "cells": [
    {
      "source": [
        "# CNN Model (Notebook version)\n",
        "## 1) Data Pre-processing\n",
        "Import the libraries"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMALnUMlrwqo"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import itertools\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters:\n",
        "NUMBER = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "ALPHABET = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
        "TABLE = NUMBER+ALPHABET # The table for the captcha\n",
        "LEN_OF_TABLE=36\n",
        "BATCH_SIZE=100\n",
        "LEN_OF_CAPTCHA=6\n",
        "LEARNING_RATE=0.001"
      ]
    },
    {
      "source": [
        "Load the data, please modify the path by yourself"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "ovGKMs-RtRkj",
        "outputId": "117a93c3-30ef-4f64-c070-1e1ab59b41db"
      },
      "source": [
        "# Load the data from the Google Drive\n",
        "# data_dir = Path(\"/content/drive/MyDrive/Data\")\n",
        "\n",
        "# path of data set for local\n",
        "data_dir = Path(\"D:/Task/FS\")\n",
        "\n",
        "images = list(data_dir.glob(\"*.jpg\")) #the size of dataset\n",
        "print(\"Number of images found: \", len(images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "The CAPTCHA samples in the data set"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_images =images[:6] \n",
        "_, ax = plt.subplots(2, 3, figsize=(20, 5))\n",
        "for i in range(6):\n",
        "    img = cv2.imread(str(sample_images[i]))\n",
        "    print(\"Shape of image: \", img.shape)\n",
        "    ax[i//3, i % 3].imshow(img)\n",
        "plt.show()"
      ]
    },
    {
      "source": [
        "### Customize the data set class"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the captcha into the (6*36,) vector (6 characters,10 numbers + 26 Upper case characters)\n",
        "# 1 means the captcha has this character in TABLE, 0 means not\n",
        "def captcha_to_vector(captcha_str):\n",
        "    captcha_str = captcha_str.upper()\n",
        "    vector = np.zeros(36*6, dtype=np.float32)\n",
        "    for i, char in enumerate(captcha_str):\n",
        "        ascii = ord(char) # Convert char into ascii code\n",
        "        if 48 <= ascii <= 57:# for numbers\n",
        "            index = ascii-48\n",
        "        elif 65 <= ascii <= 90:# for characters\n",
        "            index = ascii-ord('A')+10\n",
        "        vector[i*LEN_OF_TABLE+index] = 1.0\n",
        "    return vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the vector into the captcha (the input vector is different from the vector above)\n",
        "# example: \n",
        "#   input:[1,2,34,2,6,7] \n",
        "#   output:\"23Y378\"\n",
        "def vector_to_captcha(vector):\n",
        "    captcha_str = \"\"\n",
        "    for i in vector:\n",
        "        captcha_str += TABLE[i]\n",
        "    return captcha_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_matrix(preds,label,conf_matrix):\n",
        "    for p,t in zip(preds,label):\n",
        "        conf_matrix[TABLE.index(p),TABLE.index(t)]+=1\n",
        "    return conf_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom dataset\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images, transform=None, target_transform=None, height=50, width=200):\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.images = images\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # get the image with path\n",
        "        image = cv2.imread(str(self.images[index]))\n",
        "        label = captcha_to_vector(self.images[index].name.split(\"_\")[0])\n",
        "        # Apply the transform to the image\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ]
    },
    {
      "source": [
        "Split and create data sets"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random.shuffle(images)\n",
        "# test data\n",
        "test_data = images[8000:]  # last 2000 images in data set are for for test\n",
        "\n",
        "# the part for training\n",
        "training = images[:8000]\n",
        "valid_data = training[6000:]  # last 2000 images in training set are for validation\n",
        "train_data = training[:6000]  # 6000 images for train\n",
        "\n",
        "print(\"test set size:\", len(test_data))\n",
        "print(\"validation set size:\", len(valid_data))\n",
        "print(\"train set size:\", len(train_data))\n",
        "\n",
        "train_set = CustomDataset(train_data, transform=transforms.ToTensor())\n",
        "valid_set = CustomDataset(valid_data, transform=transforms.ToTensor())\n",
        "test_set = CustomDataset(test_data, transform=transforms.ToTensor())\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(dataset=test_set, batch_size=1, shuffle=True)\n",
        "valid_dataloader = DataLoader(dataset=valid_set, batch_size=1, shuffle=True)"
      ]
    },
    {
      "source": [
        "## 2) CNN Model"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 48, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(48),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(48, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.layer5 = nn.Linear(64*3*12, 512)\n",
        "        self.out = nn.Linear(512, 36*6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)  # Input: torch.Size([100, 1, 50, 200])\n",
        "        x = self.layer2(x)  # Input: torch.Size([100, 32, 25, 100])\n",
        "        x = self.layer3(x)  # Input: torch.Size([100, 48, 12, 50])\n",
        "        x = self.layer4(x)  # Input: torch.Size([100, 64, 6, 25])\n",
        "\n",
        "        # Output: torch.Size([100, 64, 3, 12])\n",
        "        x = x.view(-1, 64*3*12)\n",
        "        x = self.layer5(x)\n",
        "        output = self.out(x)\n",
        "        # Output: torch.Size([100, 36*6])\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the model \n",
        "model = CNN().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def valid(model, valid_dataloader, device):\n",
        "    num_correct = 0  # the counter for the correct items\n",
        "    num_total = len(valid_dataloader)  # the counter for the total items\n",
        "    model.eval()  # set the evaluation state of the model\n",
        "    with torch.no_grad():\n",
        "        for _, (images, labels) in enumerate(valid_dataloader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            output = model(images)\n",
        "            labels = labels.reshape(6, 36)\n",
        "            output = output.reshape(6, 36)\n",
        "            # get the captcha character index\n",
        "            labels = torch.argmax(labels, dim=1)\n",
        "            # get the predict character index\n",
        "            output = torch.argmax(output, dim=1)\n",
        "            num_correct += ((labels == output).sum() == 6).sum().item()\n",
        "        accuracy = num_correct / num_total * 100\n",
        "        return accuracy\n",
        "\n",
        "\n",
        "def test(model, test_dataloader, device,conf_matrix):\n",
        "    num_correct = 0  # the counter for the correct items\n",
        "    num_total = len(test_dataloader)  # the counter for the total items\n",
        "    model.eval()  # set the evaluation state of the model\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, (images, labels) in enumerate(test_dataloader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            output = model(images)\n",
        "            labels = labels.reshape(6, 36)\n",
        "            output = output.reshape(6, 36)\n",
        "            # get the captcha character index\n",
        "            labels = torch.argmax(labels, dim=1)\n",
        "            # get the predict character index\n",
        "            output = torch.argmax(output, dim=1)\n",
        "            num_correct += ((labels == output).sum() == 6).sum().item()\n",
        "            # New\n",
        "            conf_matrix=update_matrix(vector_to_captcha(output),vector_to_captcha(labels),conf_matrix)\n",
        "\n",
        "        accuracy = num_correct / num_total * 100\n",
        "        return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, train_dataloader, valid_dataloader, device):\n",
        "    model.train()\n",
        "    criterion = nn.MultiLabelSoftMarginLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    for epoch in range(15):\n",
        "        model.train()\n",
        "        for i, (images, labels) in enumerate(train_dataloader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            predict = model(images)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(predict, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        accuracy = valid(model, valid_dataloader, device)\n",
        "        print(\"epoch: {} loss: {:.10f} accuracy: {:.4f}\".format((epoch+1), loss.item(), accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\nTraining\")\n",
        "train(model, train_dataloader, valid_dataloader, device)\n",
        "\n",
        "conf_matrix=torch.zeros(36,36)\n",
        "\n",
        "print(\"\\nTesting\")\n",
        "accuracy = test(model, test_dataloader, device, conf_matrix)\n",
        "print(\"Accuracy: {}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# _, ax = plt.subplots(2, 3, figsize=(20, 5))\n",
        "with torch.no_grad():\n",
        "    for i in range(5):\n",
        "        image, label = next(iter(test_dataloader))\n",
        "\n",
        "        image = image.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model(image)\n",
        "        label = label.reshape(6, 36)\n",
        "        output = output.reshape(6, 36)\n",
        "        label = torch.argmax(label, dim=1)\n",
        "        output = torch.argmax(output, dim=1)\n",
        "        origin = vector_to_captcha(label)\n",
        "        predict = vector_to_captcha(output)\n",
        "\n",
        "        print(\"Origin: \"+origin+\"   Predict: \"+predict)"
      ]
    },
    {
      "source": [
        "## Draw the confusion matrix\n",
        "Reference：https://www.daimajiaoliu.com/daima/4ed46a79a900402"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "    print(cm)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    \n",
        "    plt.axis(\"equal\")\n",
        "    ax = plt.gca()\n",
        "    left, right = plt.xlim()\n",
        "    ax.spines['left'].set_position(('data', left))\n",
        "    ax.spines['right'].set_position(('data', right))\n",
        "    for edge_i in ['top', 'bottom', 'right', 'left']:\n",
        "        ax.spines[edge_i].set_edgecolor(\"white\")\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        num = '{:.2f}'.format(cm[i, j]) if normalize else int(cm[i, j])\n",
        "        plt.text(j, i, num,\n",
        "                 verticalalignment='center',\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if num > thresh else \"black\")\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_confusion_matrix(conf_matrix.numpy(),TABLE)"
      ]
    }
  ]
}