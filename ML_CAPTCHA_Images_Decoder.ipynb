{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_CAPTCHA_Images_Decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python385jvsc74a57bd09e57da381150c32bc445a617f5277ea2102eb0088f85f33ec3bf81d0c63bfe54",
      "display_name": "Python 3.8.5 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TdxMUK30K1Y"
      },
      "source": [
        "# **ML `CAPTCHA` Images Decoder - Winter 2021 CMPUT 466 Project**\n",
        "Group Members & Authors:\n",
        "*   Yuxi Chen\n",
        "*   Zijie Tan\n",
        "*   Lijiangnan Tian\n",
        "*   Ze Hui Peng\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLZDFkyA7DLg"
      },
      "source": [
        "## **1) Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJsH-MWoJflC"
      },
      "source": [
        "Import all necessary libraries, set the device and parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqK5xHrPz7UK",
        "outputId": "768469bf-c381-4562-f9a0-d9d23d1a5c38"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import string\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn # for CNN and RNN\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier # for KNN\n",
        "from sklearn.svm import SVC # for SVM\n",
        "from sklearn import metrics\n",
        "from google.colab import drive\n",
        "\n",
        "# Use either CUDA with torch or CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "\n",
        "# Parameters:\n",
        "NUMBERS = list(string.digits)\n",
        "ALPHABET = list(string.ascii_uppercase) # all lowercase characters will be converted to uppercase\n",
        "TABLE = NUMBERS + ALPHABET # The table for the captcha\n",
        "LEN_OF_TABLE = len(TABLE)\n",
        "BATCH_SIZE = 100\n",
        "LEN_OF_CAPTCHA = 6\n",
        "LEARNING_RATE = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyBre1emHihi"
      },
      "source": [
        "Load the data from either Google Drive or local directory\n",
        "\n",
        "Please **modify the *data_dir* variable by yourself based on where the dataset is stored**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHXhItsQ8A88"
      },
      "source": [
        "# Load the data from the Google Drive\n",
        "drive.mount('/gdrive')\n",
        "data_dir = Path(\"/gdrive/My Drive/Data/\")\n",
        "\n",
        "# path of data set for local\n",
        "# data_dir = Path(\"D:/Task/FS\")\n",
        "\n",
        "images = list(data_dir.glob(\"*.jpg\"))\n",
        "print(\"Number of images found: \", len(images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYuHTl6aJx_Z"
      },
      "source": [
        "## **2) Example of CAPTCHA Images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dYRBRTHIKFi"
      },
      "source": [
        "sample_images = images[:LEN_OF_CAPTCHA] \n",
        "_, ax = plt.subplots(2, 3, figsize=(20, 5))\n",
        "for i in range(LEN_OF_CAPTCHA):\n",
        "    img = cv2.imread(str(sample_images[i]))\n",
        "    ax[i//3, i % 3].imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAW8qLtniFcb"
      },
      "source": [
        "## **3) Preprocessing**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l728phtfIJ8V"
      },
      "source": [
        "'''\n",
        "Convert the captcha into the (6 * 36,) vector (6 characters, 10 numbers + 26 uppercase characters)\n",
        "1 means the captcha has this character in TABLE, 0 means not\n",
        "Arg:\n",
        "  captcha_str: the string that represents the sequence in the captcha image\n",
        "Return:\n",
        "  vector: a (6 * 36,) one-hot vector \n",
        "'''\n",
        "def captcha_to_vector(captcha_str):\n",
        "    captcha_str = captcha_str.upper()\n",
        "    vector = np.zeros(36 * LEN_OF_CAPTCHA, dtype = np.float32)\n",
        "    for i, char in enumerate(captcha_str):\n",
        "        ascii = ord(char)\n",
        "        if ord('0') <= ascii <= ord('9'): # for numbers\n",
        "            index = ascii - ord('0')\n",
        "        elif ord('A') <= ascii <= ord('Z'): # for characters\n",
        "            index = ascii - ord('A') + len(NUMBERS)\n",
        "        vector[i * LEN_OF_TABLE + index] = 1.0\n",
        "    return vector\n",
        "    \n",
        "'''\n",
        "Convert the vector into the captcha (the input vector is different from the vector above)\n",
        "Example: \n",
        "  input: [1,2,34,2,6,7] \n",
        "  output: \"23Y378\"\n",
        "Arg:\n",
        "  vector: a vector of where each element represents a digit or uppercase letter\n",
        "          element 0-9 represents digits 0-9, element 10-35 represent uppercase letters A-Z\n",
        "Return:\n",
        "  captcha_str: a string that decodes the vector element to its true representation\n",
        "'''\n",
        "def vector_to_captcha(vector):\n",
        "    captcha_str = \"\"\n",
        "    for i in vector:\n",
        "        captcha_str += TABLE[i]\n",
        "    return captcha_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kiCqb4Lku3J"
      },
      "source": [
        "We have three different kinds of datasets, one is used for **CNN**, one is for **RNN**, and one is for segmentation-based algorithms (**SVM** and **KNN**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jDOq42ikZgW"
      },
      "source": [
        "# Custom dataset used for CNN\n",
        "class CustomDatasetCNN(torch.utils.data.Dataset):\n",
        "    def __init__(self, images, transform=None, target_transform=None, height=50, width=200):\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.images = images\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # get the image with path\n",
        "        image = cv2.imread(str(self.images[index]))\n",
        "        label = captcha_to_vector(self.images[index].name.split(\"_\")[0])\n",
        "        # Apply the transform to the image\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "# Custom dataset used for RNN\n",
        "class CustomDatasetRNN(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # 3 chanels\n",
        "        ])\n",
        "        self.imgs = []\n",
        "        self.labels = []\n",
        "        self.imgs = dataset\n",
        "        for img in self.imgs:\n",
        "            self.labels.append(img.name.split(\"_\")[0])\n",
        "        self.l2 = list(string.ascii_uppercase + string.digits)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.imgs[index]\n",
        "        img = Image.open(img_path)\n",
        "        img = self.transform(img)\n",
        "        label = self.labels[index]\n",
        "        label = self.one_hot(label)\n",
        "        return img, label\n",
        "\n",
        "    def one_hot(self, label):\n",
        "        vector = torch.zeros(6, 36)\n",
        "        for i in range(6):\n",
        "            index = self.l2.index(label[i].upper())\n",
        "            vector[i][index] = 1\n",
        "        return vector\n",
        "\n",
        "# Custom dataset used for segmentation-based algorithms (i.e. SVM and KNN)\n",
        "class CustomDatasetSegBased(torch.utils.data.Dataset):\n",
        "    def __init__(self, images, transform=None, target_transform=None, height=50, width=200):\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.images = images\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.character_space = 25\n",
        "    \n",
        "    def noise_remover(self, image):\n",
        "        # increase contrast: segmentation-based so the preprocessing is more complicated\n",
        "        image = cv2.convertScaleAbs(image, alpha=3, beta=40)\n",
        "        # Erode noise\n",
        "        kernel = np.ones((1, 1), np.uint8)\n",
        "        image = cv2.erode(image, kernel, iterations=1)\n",
        "        # convert the image into grayscale\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGBA2GRAY)\n",
        "        # resize the image to ensure the size\n",
        "        image = cv2.resize(image, (self.width, self.height))\n",
        "        # Binarization of images\n",
        "        _, image = cv2.threshold(image, 20, 255, cv2.THRESH_BINARY|cv2.THRESH_OTSU)\n",
        "        # Method from dsp.stackexchange.com/questions/52089/removing-noisy-lines-from-image-opencv-python\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,3))\n",
        "        image = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
        "        # Shear transformation, from thepythoncode.com/article/image-transformations-using-opencv-in-python#Image_Shearing\n",
        "        M = np.float32([[1, -0.5, 0],\n",
        "             \t          [0,    1, 0],\n",
        "            \t          [0,    0, 1]])\n",
        "        rows, cols = image.shape #(50, 200)\n",
        "        image = cv2.warpPerspective(image,M,(int(cols),int(rows)), cv2.INTER_LINEAR, borderValue=(255, 255, 255))\n",
        "        # horizontal stretch\n",
        "        M = np.float32([[1.2, 0, 0],\n",
        "             \t          [0,   1, 0],\n",
        "            \t          [0,   0, 1]])\n",
        "        rows, cols = image.shape # (50, 200)\n",
        "        image = cv2.warpPerspective(image,M,(int(cols),int(rows)), cv2.INTER_LINEAR, borderValue=(255, 255, 255))\n",
        "        return image\n",
        "\n",
        "    def character_segmenter(self, image, index):\n",
        "        label = captcha_to_vector(self.images[index].name.split(\"_\")[0])\n",
        "        img_seg_list = []\n",
        "        label_list = []\n",
        "        # segmentation [image[:,:50], image[:,50:75], image[:,75:100], image[:,100:125], image[:,125:150], image[:,150:]]\n",
        "        for j in range(LEN_OF_CAPTCHA):\n",
        "          left = (j + 1) * self.character_space\n",
        "          right = (j + 2) * self.character_space\n",
        "          im_seg = image[:, left:right]\n",
        "\n",
        "          # Apply the transform to the image\n",
        "          if self.transform is not None:\n",
        "            img_seg_list.append(self.transform(im_seg))\n",
        "          else:\n",
        "            img_seg_list.append(im_seg)\n",
        "\n",
        "          label_list.append(label[j * LEN_OF_TABLE: (j + 1) * LEN_OF_TABLE])\n",
        "        return img_seg_list, label_list\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # get the image with path\n",
        "        image = cv2.imread(str(self.images[index]))\n",
        "\n",
        "        # remove noise from image\n",
        "        image = self.noise_remover(image)\n",
        "\n",
        "        # apply character segmentation\n",
        "        (img_seg_list, label_list) = self.character_segmenter(image, index)\n",
        "        return img_seg_list, label_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4oz5CXk2AmZ"
      },
      "source": [
        "## **4) Separate Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTNFlARxnGrW"
      },
      "source": [
        "# using 80% of the data for training, 20% for validation\n",
        "train_data, test_data = train_test_split(images, test_size = 0.2, random_state = 6, shuffle = True)\n",
        "\n",
        "# for segmentation-free algorithms(Neural Networks), 25% of the training data are used for validations\n",
        "train_data_segfree, validation_data = train_test_split(train_data, test_size = 0.25, random_state = 6, shuffle = True)\n",
        "\n",
        "print(\"test set size:\", len(test_data))\n",
        "print(\"validation set size:\", len(validation_data))\n",
        "print(\"train set size for segmentation-free algorithms:\", len(train_data_segfree))\n",
        "print(\"train set size for segmentation-based algorithms:\", len(train_data))\n",
        "\n",
        "# CustomDataset and DataLoader class for CNN\n",
        "train_set_cnn = CustomDatasetCNN(train_data_segfree, transform = transforms.ToTensor())\n",
        "valid_set_cnn = CustomDatasetCNN(validation_data, transform = transforms.ToTensor())\n",
        "test_set_cnn = CustomDatasetCNN(test_data, transform = transforms.ToTensor())\n",
        "train_dataloader_cnn = DataLoader(dataset = train_set_cnn, batch_size = BATCH_SIZE, shuffle = True)\n",
        "test_dataloader_cnn = DataLoader(dataset = valid_set_cnn, batch_size = 1, shuffle = True)\n",
        "valid_dataloader_cnn = DataLoader(dataset = test_set_cnn, batch_size = 1, shuffle = True)\n",
        "\n",
        "# CustomDataset and DataLoader class for RNN\n",
        "train_set_rnn = CustomDatasetRNN(train_data_segfree)\n",
        "valid_set_rnn = CustomDatasetRNN(validation_data)\n",
        "test_set_rnn = CustomDatasetRNN(test_data)\n",
        "train_dataloader_rnn = DataLoader(dataset = train_set_rnn, batch_size = BATCH_SIZE, shuffle = True)\n",
        "test_dataloader_rnn = DataLoader(dataset = valid_set_rnn, batch_size = 1, shuffle = True)\n",
        "valid_dataloader_rnn = DataLoader(dataset = test_set_rnn, batch_size = 1, shuffle = True)\n",
        "\n",
        "# CustomDataset and DataLoader class for segmentation-based algorithms (SVM and KNN)\n",
        "train_set_segbased = CustomDatasetSegBased(train_data, transform=transforms.ToTensor())\n",
        "test_set_segbased = CustomDatasetSegBased(test_data, transform=transforms.ToTensor())\n",
        "\n",
        "train_dataloader_segbased = DataLoader(dataset = train_set_segbased, batch_size = 1, shuffle = True)\n",
        "test_dataloader_segbased  = DataLoader(dataset = test_set_segbased,  batch_size = 1, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kItxMGT45bjT"
      },
      "source": [
        "## **5) Example of CAPTCHA Images after preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8VqsUPf66xU"
      },
      "source": [
        "Example image for **segmentation-based** dataset preprocessing\n",
        "\n",
        "(Original $\\to$ Erosion $\\to$ Grayscale $\\to$ Binarization $\\to$ Morph $\\to$ Shear $\\to$ Stretch $\\to$ Segmentation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufWT5yeZ7YrH"
      },
      "source": [
        "_, ax = plt.subplots(2, 3, figsize=(8, 5))\n",
        "i = 0\n",
        "img, lbl = next(iter(test_dataloader_segbased))\n",
        "for image, label in zip(img, lbl):\n",
        "    image = image.to(device)\n",
        "    label = label.to(device)\n",
        "    label = label.reshape(1, LEN_OF_TABLE)\n",
        "    label = torch.argmax(label, dim=1)\n",
        "    origin = vector_to_captcha(label).lower()\n",
        "\n",
        "    image = image.reshape(image.shape[2], image.shape[3]).cpu()\n",
        "    ax[i // 3, i % 3].imshow(image, cmap=plt.cm.gray)\n",
        "    ax[i // 3, i % 3].title.set_text(origin)\n",
        "    i += 1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akqz2h3cFJyx"
      },
      "source": [
        "## **6) Segmentation-Free Algorithms**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aCHSmgtiAou"
      },
      "source": [
        "### **6.1) Neural Network Classifier**\n",
        "A common classifier used for both Convolutional Neural Network (**CNN**) and Recurrent Neural Network (**RNN**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7fUJHIR_pgb"
      },
      "source": [
        "class NNClassifier():\n",
        "    def __init__(self, epoch, criterion, learning_rate):\n",
        "        self.epoch = epoch\n",
        "        self.criterion = criterion\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def train(self, model, train_dataloader, valid_dataloader, device):\n",
        "        model.train()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = self.learning_rate)\n",
        "        for epoch in range(self.epoch):\n",
        "            model.train()\n",
        "            for i, (images, labels) in enumerate(train_dataloader):\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                predict = model(images)\n",
        "                optimizer.zero_grad()\n",
        "                loss = self.criterion(predict, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            accuracy = self.test_and_validate(model, valid_dataloader, device)\n",
        "            print(\"Epoch: {}\\tLoss: {:.10f}\\tAccuracy: {:.2f}%\".format((epoch + 1), loss.item(), accuracy))\n",
        "  \n",
        "    def test_and_validate(self, model, dataloader, device):\n",
        "        num_correct = 0  # the counter for the correct items\n",
        "        num_total = len(dataloader)  # the counter for the total items\n",
        "        model.eval()  # set the evaluation state of the model\n",
        "        with torch.no_grad():\n",
        "            for _, (images, labels) in enumerate(dataloader):\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                output = model(images)\n",
        "                labels = labels.reshape(LEN_OF_CAPTCHA, LEN_OF_TABLE)\n",
        "                output = output.reshape(LEN_OF_CAPTCHA, LEN_OF_TABLE)\n",
        "                # get the captcha character index\n",
        "                labels = torch.argmax(labels, dim=1)\n",
        "                # get the predict character index\n",
        "                output = torch.argmax(output, dim=1)\n",
        "                num_correct += ((labels == output).sum() == 6).sum().item()\n",
        "            accuracy = num_correct / num_total * 100\n",
        "            return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_jQFZCNGGpi"
      },
      "source": [
        "### **6.2) Covolutional Neural Network (CNN)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYFwwoJOGMw4"
      },
      "source": [
        "CNN Model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1W63FrE-vd7"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 48, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(48),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(48, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.layer5 = nn.Linear(64*3*12, 512)\n",
        "        self.out = nn.Linear(512, 36*6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)  \n",
        "        x = self.layer2(x)  \n",
        "        x = self.layer3(x)  \n",
        "        x = self.layer4(x)\n",
        "\n",
        "        # Output: torch.Size([100, 64, 3, 12])\n",
        "        x = x.view(-1, 64*3*12)\n",
        "        x = self.layer5(x)\n",
        "        output = self.out(x)\n",
        "        # Output: torch.Size([100, 36*6])\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLG8AsOzA-5P"
      },
      "source": [
        "Training and Testing CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qc0S-mFA-Ie"
      },
      "source": [
        "# Initialize the model \n",
        "model = CNN().to(device)\n",
        "\n",
        "# Initialize the classifier\n",
        "cnnClf = NNClassifier(15, nn.MultiLabelSoftMarginLoss(), LEARNING_RATE)\n",
        "\n",
        "print(\"CNN Training\")\n",
        "cnnClf.train(model, train_dataloader_cnn, valid_dataloader_cnn, device)\n",
        "\n",
        "print(\"CNN Testing\")\n",
        "accuracy = cnnClf.test_and_validate(model, test_dataloader_cnn, device)\n",
        "print(\"Accuracy: {}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqjGvmPZHEyi"
      },
      "source": [
        "#### **Example predictions of CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEnCDSp2HEHI"
      },
      "source": [
        "model.eval()  # set the evaluation state of the model\n",
        "with torch.no_grad():\n",
        "    for i in range(6):\n",
        "        image, label = next(iter(test_dataloader_cnn))\n",
        "\n",
        "        image = image.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model(image)\n",
        "        label = label.reshape(LEN_OF_CAPTCHA, LEN_OF_TABLE)\n",
        "        output = output.reshape(LEN_OF_CAPTCHA, LEN_OF_TABLE)\n",
        "        label = torch.argmax(label, dim=1)\n",
        "        output = torch.argmax(output, dim=1)\n",
        "        origin = vector_to_captcha(label)\n",
        "        predict = vector_to_captcha(output)\n",
        "\n",
        "        print(\"Origin: \"+origin+\"   Predict: \"+predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV59ClSFRpfK"
      },
      "source": [
        "###  **6.3) Recurrent Neural Network (RNN)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuEv19KOTueh"
      },
      "source": [
        "RNN Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_-rK1ezQ_uK"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(150, 128),\n",
        "            nn.BatchNorm1d(num_features=128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.lstm = nn.LSTM(input_size=200*128//5,\n",
        "                            hidden_size=256,\n",
        "                            num_layers=2,\n",
        "                            batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (128, 3, 200, 50) -> (128, 150, 200)\n",
        "        x = x.reshape(-1, 150, 200).permute(0, 2, 1)\n",
        "        x = x.reshape(-1, 150)\n",
        "        fc1 = self.fc1(x)\n",
        "        fc1 = fc1.reshape(-1, 5, 200*128//5)\n",
        "        lstm, (h_n, h_c) = self.lstm(fc1, None)\n",
        "        out = lstm[:, -1, :]\n",
        "        return out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=256,\n",
        "                            hidden_size=128,\n",
        "                            num_layers=2,\n",
        "                            batch_first=True)\n",
        "        self.out = nn.Linear(6 * 128, 6 * 36)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (N, 256) -> (N, 6, 256) -> (N, 6, 128) -> (N, 6*128) -> (N, 6*36) -> (N, 6, 36)\n",
        "        x = x.reshape(-1, 1, 256) \n",
        "        x = x.expand(-1, 6, 256)\n",
        "        lstm, (h_n, h_c) = self.lstm(x, None)\n",
        "        y1 = lstm.reshape(-1, 6*128)\n",
        "        out = self.out(y1)\n",
        "        output = out.reshape(-1, 6, 36)\n",
        "        return output\n",
        "\n",
        "\n",
        "class RNN (nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNN, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoder = self.encoder(x)\n",
        "        decoder = self.decoder(encoder)\n",
        "        return decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RxL-iOcT4i9"
      },
      "source": [
        "Training and Testing RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZvd6IMgT675"
      },
      "source": [
        "# Initialize the model \n",
        "model = RNN().to(device)\n",
        "\n",
        "# Initialize the classifier\n",
        "rnnClf = NNClassifier(60, nn.MSELoss(), LEARNING_RATE)\n",
        "\n",
        "print(\"RNN Training\")\n",
        "rnnClf.train(model, train_dataloader_rnn, valid_dataloader_rnn, device)\n",
        "\n",
        "print(\"RNN Testing\")\n",
        "accuracy = rnnClf.test_and_validate(model, test_dataloader_rnn, device)\n",
        "print(\"Accuracy: {}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_mPytIlhQaq"
      },
      "source": [
        "#### **Example predictions of RNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJgznkjXhW9h"
      },
      "source": [
        "model.eval()  # set the evaluation state of the model\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(6):\n",
        "        image, label = next(iter(test_dataloader_rnn))\n",
        "\n",
        "        image = image.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model(image)\n",
        "        label = label.reshape(6, 36)\n",
        "        output = output.reshape(6, 36)\n",
        "        label = torch.argmax(label, dim=1)\n",
        "        output = torch.argmax(output, dim=1)\n",
        "        origin = vector_to_captcha(label)\n",
        "        predict = vector_to_captcha(output)\n",
        "        print(\"Origin: \"+origin+\"   Predict: \"+predict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neEoGvuHiVsi"
      },
      "source": [
        "## **7) Segmentation-Based Algorithms**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZTRuzQMXBRV"
      },
      "source": [
        "### **7.1) Helper functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NGAfbfSXMnP"
      },
      "source": [
        "def get_data(dataloader):\n",
        "  X = []\n",
        "  Y = []\n",
        "  n = len(dataloader)\n",
        "  for z in range(n):\n",
        "    img, lbl = next(iter(dataloader))\n",
        "    for image, label in zip(img, lbl):\n",
        "      image = image.to(device)\n",
        "      label = label.to(device)\n",
        "      label = label.reshape(1, LEN_OF_TABLE)\n",
        "      label = torch.argmax(label, dim=1)\n",
        "      label = vector_to_captcha(label)\n",
        "      image = image.reshape(image.shape[2], image.shape[3]).cpu()\n",
        "      X.append(image.flatten().tolist())\n",
        "      Y.append(label)\n",
        "  return X, Y\n",
        "\n",
        "# Groups the original and the predicted characters together to into CAPTCHAs\n",
        "def group(old_list):\n",
        "    n = len(old_list)\n",
        "    new_list = []\n",
        "    for i in range(0, n, LEN_OF_CAPTCHA):\n",
        "        captcha = old_list[i:i+LEN_OF_CAPTCHA]\n",
        "        new_list.append(''.join(captcha))\n",
        "    return new_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEuD7bHHWhfs"
      },
      "source": [
        "### **7.2) Support Vector Machine (SVM)**\n",
        "Training and Testing SVM using `SVC` class from *sklearn*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtekFPjnduRw"
      },
      "source": [
        "# SVM code is modified from kaggle.com/sanesanyo/digit-recognition-using-svm-with-98-accuracy\n",
        "# Creating the linear SVM object\n",
        "clf = SVC(C=1,kernel=\"linear\")\n",
        "\n",
        "print(\"SVM Training\")\n",
        "train_x, train_y = get_data(train_dataloader_segbased)\n",
        "clf.fit(train_x, train_y)\n",
        "\n",
        "print(\"SVM Testing\")\n",
        "test_x, test_y = get_data(test_dataloader_segbased)\n",
        "y_predict = clf.predict(test_x)\n",
        "# Group the original and the predicted characters together to a CAPTCHA\n",
        "test_y = group(test_y)\n",
        "y_predict = group(y_predict)\n",
        "\n",
        "accuracy = metrics.accuracy_score(test_y, y_predict)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho2NEdYKhfXq"
      },
      "source": [
        "#### **Example predictions of SVM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxOMqlSrjOBJ"
      },
      "source": [
        "_, ax = plt.subplots(2, 3, figsize=(20, 5))\n",
        "for p in range(LEN_OF_CAPTCHA):\n",
        "    i, l = next(iter(test_dataloader_segbased))\n",
        "    img = np.zeros((50,150))\n",
        "    j = 0\n",
        "    predicts = []\n",
        "    origins = []\n",
        "    for image, label in zip(i, l):\n",
        "        image = image.to(device)\n",
        "        image = image.reshape(1,image.shape[2]* image.shape[3]).cpu()\n",
        "        predict = clf.predict(image)\n",
        "        predicts.append(predict[0])\n",
        "        img[:,j:j+25] = image.reshape(50,25)\n",
        "        j += 25\n",
        "        label = label.to(device)\n",
        "        label = label.reshape(1, 36)\n",
        "        label = torch.argmax(label, dim=1)\n",
        "        origin = vector_to_captcha(label)\n",
        "        origins.append(origin)\n",
        "    ax[p // 3, p % 3].imshow(img,cmap=plt.cm.gray)\n",
        "    ax[p // 3, p % 3].title.set_text(\"Original: \"+\"\".join(origins)+\"    Predicted: \"+\"\".join(predicts))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rDafWSciopR"
      },
      "source": [
        "### **7.3) $k$-Nearest Neigbours (KNN)**\n",
        "Training and Testing KNN using `KNeighborsClassifier` class from *sklearn*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GXQ9e6lji_BK"
      },
      "source": [
        "train_x, train_y = get_data(train_dataloader_segbased)\n",
        "test_x, test_y = get_data(test_dataloader_segbased)\n",
        "test_y = group(test_y)\n",
        "\n",
        "for i in range(1, 50):\n",
        "    knn_clf = KNeighborsClassifier(n_neighbors = i, weights = \"uniform\", n_jobs=-1)\n",
        "    print(\"Training KNN with k = {}\".format(i))\n",
        "    knn_clf.fit(train_x, train_y)\n",
        "    print(\"Testing KNN with k = {}\".format(i))\n",
        "    y_predict = knn_clf.predict(test_x)\n",
        "    y_predict = group(y_predict)\n",
        "\n",
        "    accuracy=metrics.accuracy_score(test_y, y_predict)\n",
        "    print(\"number of neighbors = {}, Accuracy: {:.2f}%\".format(i, accuracy*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s5QFDiNio67"
      },
      "source": [
        "#### **Example predictions of KNN**\n",
        "with $k = 5$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NcnjghAr9DB"
      },
      "source": [
        "knn_clf = KNeighborsClassifier(n_neighbors = 5, weights = \"uniform\")\n",
        "train_x, train_y = get_data(train_dataloader_segbased) # run this if previous cell has not been run\n",
        "knn_clf.fit(train_x, train_y)\n",
        "\n",
        "_, ax = plt.subplots(2, 3, figsize=(20, 5))\n",
        "for p in range(LEN_OF_CAPTCHA):\n",
        "    i, l = next(iter(test_dataloader_segbased))\n",
        "    img = np.zeros((50,150))\n",
        "    j = 0\n",
        "    predicts = []\n",
        "    origins = []\n",
        "    for image, label in zip(i, l):\n",
        "        image = image.to(device)\n",
        "        image = image.reshape(1,image.shape[2]* image.shape[3]).cpu()\n",
        "        predict = knn_clf.predict(image)\n",
        "        predicts.append(predict[0])\n",
        "        img[:,j:j+25] = image.reshape(50,25)\n",
        "        j += 25\n",
        "        label = label.to(device)\n",
        "        label = label.reshape(1, 36)\n",
        "        label = torch.argmax(label, dim=1)\n",
        "        origin = vector_to_captcha(label)\n",
        "        origins.append(origin)\n",
        "    ax[p // 3, p % 3].imshow(img,cmap=plt.cm.gray)\n",
        "    ax[p // 3, p % 3].title.set_text(\"Original: \"+\"\".join(origins)+\"    Predicted: \"+\"\".join(predicts))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}